{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- classification algorithm\n",
    "- statistical model widely used for predicting binary classes (2 possible outcomes: 0 and 1)\n",
    "- Spam detection, detection of certain diseases ...\n",
    "- computes the probability of an event occurrence using the logit/sigmoid function\n",
    "\n",
    "<img src=\"resources/logisticfunc.png\" style=\"width: 800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Excursus Linear Regression\n",
    "- algorithm used for regression problems (with a numeric outcome)\n",
    "- simple linear regression: we try to find the best linear function ax + b fitting to our data\n",
    "- a is our slope parameter, b our intercept (a and b = our \"Theta\"-weights, we try to optimize)<br>\n",
    "<img src=\"resources/linear_regression.png\" style=\"width: 800px\"><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- logistic regression bases upon linear regression \n",
    "- why do we not use linear regression for classification problems as well?  \n",
    "    1. only two possible outcomes: the linear slope doesn't map this very well\n",
    "    2. output values bigger than 1 and smaller than 0 can occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "heartdisease = pd.read_csv(\"data/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert categorical variable into dummy/indicator variables, concerns cp, thal, slope\n",
    "# See: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "a = pd.get_dummies(heartdisease['cp'], prefix = \"cp\")\n",
    "b = pd.get_dummies(heartdisease['thal'], prefix = \"thal\")\n",
    "c = pd.get_dummies(heartdisease['slope'], prefix = \"slope\")\n",
    "\n",
    "# Add the dummy columns, drop the old categorical variables \n",
    "frames = [heartdisease, a, b, c]\n",
    "df = pd.concat(frames, axis = 1).drop(columns = ['cp', 'thal', 'slope'])\n",
    "df.head()\n",
    "\n",
    "# Separate input data and output\n",
    "y = df.target.values\n",
    "x_data = df.drop(['target'], axis = 1)\n",
    "\n",
    "# Normalize the data with MinMax-Scaler function\n",
    "x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values\n",
    "\n",
    "# Split the data in trainings and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 86.89%\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Algorithm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "print(\"Test Accuracy %.2f%%\" % (lr.score(x_test,y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions vs. actual Classes:\n",
      "Pred:  [0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1]\n",
      "Class: [0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1]\n",
      "\n",
      "Prediction Probs: \n",
      "\n",
      "[[0.88944519 0.11055481]\n",
      " [0.54319211 0.45680789]\n",
      " [0.52339135 0.47660865]\n",
      " [0.95430018 0.04569982]\n",
      " [0.83296797 0.16703203]\n",
      " [0.75869545 0.24130455]\n",
      " [0.91481407 0.08518593]\n",
      " [0.89223814 0.10776186]\n",
      " [0.95133633 0.04866367]\n",
      " [0.97491908 0.02508092]\n",
      " [0.36443036 0.63556964]\n",
      " [0.10378138 0.89621862]\n",
      " [0.92980393 0.07019607]\n",
      " [0.10644516 0.89355484]\n",
      " [0.0603269  0.9396731 ]\n",
      " [0.25492222 0.74507778]\n",
      " [0.89874266 0.10125734]\n",
      " [0.18347126 0.81652874]\n",
      " [0.97463813 0.02536187]\n",
      " [0.26027727 0.73972273]]\n"
     ]
    }
   ],
   "source": [
    "prediction = lr.predict(x_test)\n",
    "pred_prob = lr.predict_proba(x_test)\n",
    "\n",
    "print(\"Predictions vs. actual Classes:\")\n",
    "print(\"Pred: \", prediction[:20])\n",
    "print(\"Class:\", y_test[:20])\n",
    "print(\"\\nPrediction Probs: \\n\")\n",
    "print(pred_prob[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-nearest Neighbor\n",
    "\n",
    "<img src=\"resources/knn.png\" style=\"width: 500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- relatively simple and often used classification algorithm (can also be used for regression)\n",
    "- we diagnose the class of a certain sample by looking at the closest known points (neighbors) around: their \"majority class\" will become our sample's class\n",
    "- hyperparameter k := number of neighbors\n",
    "- distance function, vote function \n",
    "- dataset = (x, y), x := Features, y := classes\n",
    "- in sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNeighborsClassifier\n",
    "- class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- `n_neighbors` : int, optional (default = 5). Number of neighbors to use by default for kneighbors queries\n",
    "- `weights` : str or callable, optional (default = ‘uniform’). \n",
    "    - uniform : uniform weights. All points in each neighborhood are weighted equally.\n",
    "    - distance’ : weight points by the inverse of their distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises K-nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Take two columns of your choice and assign them to a new dataframe x_train1 and x_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# KNN Algorithm\n",
    "# import the KNeighborsClassifier and save two instances for your two dataframes in a variable \"knn\" and \"knn1\". \n",
    "# We want to start with 10 neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model on your new smaller dataset\n",
    "# How does it perform? Check the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model on your old dataset with all features\n",
    "# How does it perform? Check the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Compare prediction and actual classes for one of your trained models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Bonus, if you are familiar with python:\n",
    "# We try to find the best number of neighbors between 1 and 20.\n",
    "# Realize it with a for-loop."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
